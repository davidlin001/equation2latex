# Title: encoder.py
# Author: David Lin, Cody Kala
# Date: 6/3/2018
# ===============================
# Implements the encoder stage of the translation model introduced by 
# Deng at al. Their Arxiv paper can be found here:
#
#   https://arxiv.org/pdf/1609.04938.pdf
#
# The authors have an TensorFlow implementation of their model available
# on GitHub:
#
#   https://github.com/harvardnlp/im2markup

import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
	""" The Encoder takes the feature grids processed by the CNN layer
	and produces new feature grids that will be fed to the Decoder layer.

	The new feature grids are generated by running RNNs over each of
	the rows of the CNN features. As a result, the encoder is allowed
	to localize its inputs.

	Concretely, in this model, the new feature grid V is created from
	the old feature grid U by running an RNN across each row of that
	input. Recursively for all rows h in {1, ..., H} and columns
	w in {1, ..., W}, the new features are defined as

		V_{h, w} = RNN(U_{h,w - 1}, U_{h, w})

	where RNN can be any variant of a recurrent neural network. In order
	to capture the sequential order information in the vertical direction,
	we use a trainable initial hidden state V_{h, 0} for each row, which
	we refer to as positional embeddings.
	"""

	def __init__(self, num_channels, height, cell_type="lstm"):
		""" Initializes the layers for the Encoder module. 

		Inputs:
			num_channels : int
				The number of channels in the feature grids.
			height : int
				The number of pixels along the height dimension of the 
				feature grids.
			width : int
				The number of pixels along the width dimension of the
				feature grids.
			cell_type : string
				Optional, specifies the type of RNN cell to use. Must be one
				of "rnn", "gru", or "lstm". The default value is "lstm".

		Outputs:
			None, but the layers are initialized.
		"""
		# Store the dimensions of the feature grid
		self.num_channels = num_channels
		self.height = height
		self.width = width

		# Initialize RNN cell
		if cell_type == "rnn":
			self.cell_fw = nn.RNN(num_channels, num_channels)
			self.cell_bw = nn.RNN(num_channels, num_channels)
		elif cell_type == "gru":
			self.cell_fw = nn.GRU(num_channels, num_channels)
			self.cell_bw = nn.GRU(num_channels, num_channels)
		elif cell_type == "lstm":
			self.cell_fw = nn.LSTM(num_channels, num_channels)
			self.cell_bw = nn.LSTM(num_channels, num_channels)

		# Initialize learnable hidden states for each row
		self.hiddens = torch.zeros((num_channels, height, 1), requires_grad=True)

		raise NotImplementedError

	def forward(self, x):
		""" Computes the forward pass for the Encoder layer.

		Inputs:
			U : torch.tensor of shape (batch_size, D, H, W)
				A mini-batch of feature grids produced by the CNN layer.

		Outputs:
			V : torch.tensor of shape (batch_size, D, H, W)
				A mini-batch of feature grids to be fed to the Decoder layer.
		"""
		raise NotImplementedError

