# Title: decoder.py
# Author: Cody Kala
# Date: 6/3/2018
# =====================
# This module implements the decoder stage of the translation
# model used by Guillaume Genthial and Romain Sauvestre in
# their CS231n paper, which can be found here:
#
#   cs231n.stanford.edu/reports/2017/pdfs/815.pdf

import torch
import torch.nn as nn
import torch.nn.functional as F

class Decoder(nn.Module):
    """ The Decoder decodes the feature maps generated by the Encoder layer 
    to produce the LaTeX tokens with a recurrent neural network (RNN). 
    
    Any variant of an RNN will do for this task, but by default we will use an
    LSTM since these have shown to perform very well when it comes to capturing 
    long-term dependencies and back-propagation of gradients.
    """

    def __init__(self, vocab_size, batch_size, hidden_size, max_length, cell_type="lstm"):
        """ Initializes the Decoder layer.

        Inputs:
            vocab_size : int
                The number of tokens in the vocabulary. The vocabulary size
                should also include for the START, END, and PAD tokens.
            cell_type : string
                Optional, specifies the type of RNN cell to use. Must be one
                of "rnn", "gru", or "lstm". The default value is "lstm".

        Outputs:
            None
        """
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.batch_size = batch_size
        self.max_length = max_length
        self.cell_mlp = nn.Sequential(
            nn.Linear(512, 100),
            nn.ReLU(),
            nn.Linear(100, vocab_size)
            )
        self.hidden_mlp = nn.Seqeuntial(
            nn.Linear(512, 100),
            nn.ReLU(),
            nn.Linear(100, vocab_size)
            )
        self.hidden_mlp = 
        if cell_type == "rnn":
            self.cell = nn.RNN(vocab_size, vocab_size)
        elif cell_type == "gru":
            self.cell = nn.GRU(vocab_size, vocab_size)
        elif cell_type == "lstm":
            self.cell = nn.LSTM(vocab_size, vocab_size)
        raise NotImplementedError


    def forward(self, x)
        """ Computes the forward pass for the Decoder layer. 
        
        Inputs:
            x : torch.Tensor of shape (batch_size, H', W', C)
                A mini-batch of feature maps from the Encoder layer.

        Outputs:
            out : torch.Tensor of shape (batch_size, max_length)
                A mini-batch of LaTeX decodings. Each row contains |max_length|
                int IDs that map to tokens in the vocabulary.
        """
        hidden = self.initHiddenState(x)
        cell = self.initCellState(x)
        output, hidden = self.cell(x, (hidden,cell))
        scores = self.softmax(output)

    def initHiddenState(self, x):
        #return torch.zeros(self.hidden_size, device=device, requires_grad = True)
        x = torch.mean(x, 2)
        x = torch.mean(x, 1)
        x = self.hidden_mlp(x)
        return x

    def initCellState(self, x):
        x = torch.mean(x, 2)
        x = torch.mean(x, 1)
        x = self.cell_mlp(x)
        return x


        
