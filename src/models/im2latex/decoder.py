# Title: decoder.py
# Author: Cody Kala
# Date: 6/3/2018
# =====================
# This module implements the decoder stage of the translation
# model used by Guillaume Genthial and Romain Sauvestre in
# their CS231n paper, which can be found here:
#
#   cs231n.stanford.edu/reports/2017/pdfs/815.pdf

import torch
import torch.nn as nn
import torch.nn.functional as F


class Decoder(nn.Module):
    """ The Decoder decodes the feature maps generated by the Encoder layer 
    to produce the LaTeX tokens with a recurrent neural network (RNN). 
    
    Any variant of an RNN will do for this task, but by default we will use an
    LSTM since these have shown to perform very well when it comes to capturing 
    long-term dependencies and back-propagation of gradients.
    """

    def __init__(self, vocab_size, max_length, embed_size):
        """ Initializes the Decoder layer.

        Inputs:
            vocab_size : int
                The number of tokens in the vocabulary. The vocabulary size
                should also include for the START, END, and PAD tokens.
            cell_type : string
                Optional, specifies the type of RNN cell to use. Must be one
                of "rnn", "gru", or "lstm". The default value is "lstm".

        Outputs:
            None
        """
        super().__init__()

        # Save model parameters
        self.vocab_size = vocab_size
        self.max_length = max_length

        # MLPs for cell initialization
        self.cell_mlp = nn.Sequential(
            nn.Linear(512, 100),
            nn.ReLU(),
            nn.Linear(100, vocab_size)
            )
        self.hidden_mlp = nn.Sequential(
            nn.Linear(512, 100),
            nn.ReLU(),
            nn.Linear(100, vocab_size)
            )
        self.embed_size = embed_size
        self.embedding = nn.Embedding(vocab_size, embed_size).cuda()
        self.hidden = None
        self.cell = None
        # Initialize cell type
        self.LSTM_cell = nn.LSTMCell(embed_size, vocab_size).cuda()
        
    def forward(self, x):
        """ Computes the forward pass for the Decoder layer. 
        
        Inputs:
            x : torch.Tensor of shape (batch_size, D, W', H')
                A mini-batch of feature maps from the Encoder layer.
    
        Outputs:
            out : torch.Tensor of shape (batch_size, max_length)
                A mini-batch of LaTeX decodings. Each row contains |max_length|
                int IDs that map to tokens in the vocabulary.
        """
        #hidden = Variable(hidden.data, requires_grad = True)
        #cell = Variable(cell.data, requires_grad = True)
        batch_size = x.shape[0]
        self.hidden = self.initHiddenState(x)
        self.cell = self.initCellState(x)

        # Fixing <START>, <END>, <PAD> as indices 0, 1, 2
        embedding_matrix = self.embedding(torch.LongTensor([0] * batch_size).cuda())
        embedding_matrix = embedding_matrix.view((batch_size,-1)).cuda()
        all_scores = torch.empty((self.max_length, batch_size, self.vocab_size)).cuda()
        all_indices = torch.empty((self.max_length, batch_size), dtype=torch.long).cuda()

        for i in range(0, self.max_length):
            self.hidden, self.cell = self.LSTM_cell(embedding_matrix, (self.hidden,self.cell))
            # scores = self.hidden / F.tanh(self.cell)
            scores = self.hidden
            indices = torch.argmax(scores, dim = 1)
            all_scores[i,:,:] = scores
            all_indices[i,:] = indices
            embedding_matrix = self.embedding(indices).view((batch_size, -1))
        return all_scores, all_indices

    def initHiddenState(self, x):
        #return torch.zeros(self.hidden_size, device=device, requires_grad = True)
        x = torch.mean(x, 3)
        x = torch.mean(x, 2)
        x = self.hidden_mlp(x)
        return x

    def initCellState(self, x):
        x = torch.mean(x, 3)
        x = torch.mean(x, 2)
        x = self.cell_mlp(x)
        return x


        
